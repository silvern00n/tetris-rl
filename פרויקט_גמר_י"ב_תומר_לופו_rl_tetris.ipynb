{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silvern00n/tetris-rl/blob/main/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98_%D7%92%D7%9E%D7%A8_%D7%99%22%D7%91_%D7%AA%D7%95%D7%9E%D7%A8_%D7%9C%D7%95%D7%A4%D7%95_rl_tetris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chDZFAwtdcoZ"
      },
      "source": [
        "\n",
        "<div dir = rtl>\n",
        "\n",
        "\n",
        "---\n",
        "#**טטריס לימוד חיזוק**\n",
        "---\n",
        "פרוייקט גמר תומר לופו 2025  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2dKBgs2aWM7"
      },
      "source": [
        "<div dir = rtl>\n",
        "\n",
        "# (1) אתחול והכנות"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRBq8-1yaeYU",
        "outputId": "8e69c38a-b304-4422-c96e-bb826040d265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ],
      "source": [
        "print(\"hello world\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-8R0F-pSQuM",
        "outputId": "4bf47683-db31-4b8f-f221-a3ea8ae7c288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import successful\n"
          ]
        }
      ],
      "source": [
        "# ── Standard Library ───────────────────────────────\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import pickle\n",
        "import time\n",
        "from collections import deque, namedtuple\n",
        "from itertools import count\n",
        "\n",
        "# ── Third-Party Libraries ───────────────────────────\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import imageio\n",
        "import pygame as pg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display, HTML\n",
        "print(\"import successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZPqi2_pac_x",
        "outputId": "6446ccbf-0e0e-48f8-a005-3d2b7af36d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "Python version: 3.11.12\n"
          ]
        }
      ],
      "source": [
        "python_version = sys.version_info\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Python version: {python_version.major}.{python_version.minor}.{python_version.micro}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxPIhf-8akA9",
        "outputId": "bd59b624-cfff-4378-958e-a9d1fa010506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "090g2gcE4MFO",
        "outputId": "1e948898-a38b-4833-e3b5-7f1967a85480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Version: 12.4\n",
            "CUDNN Version: 90300\n",
            "CUDNN is available: True\n"
          ]
        }
      ],
      "source": [
        "print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "print(f\"CUDNN Version: {torch.backends.cudnn.version()}\")\n",
        "print(f\"CUDNN is available: {torch.backends.cudnn.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZlUpS8gcwZ0",
        "outputId": "827f8c0a-3e1f-42f3-a0ae-f6e8f531d754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "mounted\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"mounted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u-b3SYuAetIp"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(RANDOM_SEED)\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plmuL_gecyw8",
        "outputId": "6aba72d8-9536-4417-b4e2-94eb6f708db5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Custom font not found — using system fallback.\n",
            "Successfully imported Tetris game components:\n",
            "- Game board dimensions: 10x20\n",
            "- Available Tetromino types: ['T', 'O', 'J', 'L', 'I', 'S', 'Z']\n",
            "- Game states: {'MENU': 0, 'PLAYING': 1, 'GAME_OVER': 2}\n"
          ]
        }
      ],
      "source": [
        "project_path = '/content/drive/My Drive/tetris_game'\n",
        "sys.path.append(project_path)\n",
        "\n",
        "from tetris import Tetris\n",
        "from block import Block\n",
        "from tetromino import Tetromino\n",
        "from tetris_settings import *\n",
        "from tetris_settings import stage_params\n",
        "from app import App\n",
        "from menu import Menu\n",
        "\n",
        "font_path = '/content/drive/My Drive/tetris_game/FREAKSOFNATUREMASSIVE.ttf'\n",
        "globals()['FONT_PATH'] = font_path\n",
        "\n",
        "# Display imported components\n",
        "print(\"Successfully imported Tetris game components:\")\n",
        "print(f\"- Game board dimensions: {FIELD_W}x{FIELD_H}\")\n",
        "print(f\"- Available Tetromino types: {list(TETROMINOES.keys())}\")\n",
        "print(f\"- Game states: {GAME_STATES}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-W3aKNUdiHW"
      },
      "source": [
        "<div dir = rtl>\n",
        "\n",
        "# (2) סביבת המשחק"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir = rtl>\n",
        "\n",
        "הקוד מגדיר מעטפת (Wrapper) עבור משחק טטריס שמשתמשת בפורמט של Gym כדי לאפשר למידת חיזוק. המעטפת מאפשרת לתרגם פעולה למיקום וסיבוב, לאתחל את מצב המשחק, לחשב תכונות (features) מתקדמות מהלוח (כמו חורים, מעבר בין מצבים, wells ועוד), ומיישמת מערכת תגמול מורכבת הכוללת shaping מבוסס פונקציית פוטנציאל, בונוסים על הישרדות וקומבו, ועונשים על לוח בעייתי. הקוד כולל גם יכולות רינדור לאנליזה גרפית ואופציה למצב ללא רינדור לאימון מהיר.\n",
        "\n"
      ],
      "metadata": {
        "id": "9joww7it6kvp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "84IF2nkHe0f7"
      },
      "outputs": [],
      "source": [
        "class TetrisWrapper(gym.Env):\n",
        "    \"\"\"\n",
        "    A wrapper for the Tetris game compatible with OpenAI Gym.\n",
        "    Designed for fast training with optional graphical rendering.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, render_mode=None, stage=5):\n",
        "        \"\"\"\n",
        "        Initializes the environment and Tetris game for a specific stage.\n",
        "        \"\"\"\n",
        "        # Define the action space (placement column and rotation)\n",
        "        self.action_space = self._create_action_space()\n",
        "\n",
        "        # Launch full app if rendering is requested, else use mock\n",
        "        if render_mode == \"human\":\n",
        "            self.app = App()\n",
        "            self.has_display = True\n",
        "        else:\n",
        "            self.app = self._create_mock_app()\n",
        "            self.has_display = False\n",
        "\n",
        "        # Set field dimensions according to stage settings\n",
        "        self.field_width, self.field_height = STAGE_BOARD_SIZES.get(stage, (10, 20))\n",
        "        print(f\"\\nInitializing stage {stage} with board size: {self.field_width}×{self.field_height}\")\n",
        "\n",
        "        # Update global dimensions and app-specific values\n",
        "        update_field_dimensions(self.field_width, self.field_height)\n",
        "        self.app.field_width = self.field_width\n",
        "        self.app.field_height = self.field_height\n",
        "\n",
        "        # Gamma and initial shaping potential for reward shaping\n",
        "        self.gamma = 0.999\n",
        "        self.prev_potential = 0.0\n",
        "\n",
        "        # Create the Tetris game instance\n",
        "        self.tetris = Tetris(self.app)\n",
        "\n",
        "        # Dellacherie heuristic weights for potential-based reward shaping\n",
        "        self.DELLACHERIE_W = np.array([\n",
        "            -4.500158825082766,  # aggregate height\n",
        "            +3.4181268101392694, # complete lines\n",
        "            -3.2178882868487753, # holes\n",
        "            -9.348695305445199,  # bumpiness\n",
        "        ])\n",
        "\n",
        "        # Observation space shape\n",
        "        self.observation_space_shape = self._get_state_shape()\n",
        "\n",
        "        # Game statistics\n",
        "        self.score = 0\n",
        "        self.lines_cleared = 0\n",
        "        self.pieces_placed = 0\n",
        "        self.game_over = False\n",
        "\n",
        "    def _create_mock_app(self):\n",
        "        \"\"\"Creates a minimal App object for headless (non-rendered) training.\"\"\"\n",
        "        mock_app = type('MockApp', (), {\n",
        "            'game_state': GAME_STATES['PLAYING'],\n",
        "            'allowed_shapes': list(TETROMINOES.keys()),\n",
        "            'anim_trigger': True,\n",
        "            'fast_anim_trigger': True,\n",
        "            'field_width': 10,\n",
        "            'field_height': 20,\n",
        "            'screen': None,\n",
        "            'set_field_dimensions': lambda self, w, h: update_field_dimensions(w, h)\n",
        "        })()\n",
        "        return mock_app\n",
        "\n",
        "    def _create_action_space(self):\n",
        "        \"\"\"\n",
        "        Generates all possible (column, rotation) combinations for a 10-column board.\n",
        "        \"\"\"\n",
        "        action_space = []\n",
        "        max_width = 10\n",
        "        max_rotations = 4\n",
        "\n",
        "        for column in range(max_width):\n",
        "            for rotation in range(max_rotations):\n",
        "                action_space.append((int(column), int(rotation)))\n",
        "        return action_space\n",
        "\n",
        "    def _get_state_shape(self):\n",
        "        \"\"\"\n",
        "        Returns the shape of the observation: board + current + next tetromino.\n",
        "        \"\"\"\n",
        "        field_shape = (self.field_height, self.field_width)\n",
        "        tetromino_shape = (7,)  # One-hot vector for each piece\n",
        "        return (field_shape, tetromino_shape, tetromino_shape)\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        \"\"\"\n",
        "        Resets the environment and returns the initial state.\n",
        "        \"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "            random.seed(seed)\n",
        "            torch.manual_seed(seed)\n",
        "            Tetromino.set_seed(seed)\n",
        "\n",
        "        self.tetris.reset_game()\n",
        "        self.score = 0\n",
        "        self.lines_cleared = 0\n",
        "        self.pieces_placed = 0\n",
        "        self.game_over = False\n",
        "\n",
        "        observation = self._get_state()\n",
        "        return observation, {}\n",
        "\n",
        "    def _get_state(self):\n",
        "        \"\"\"\n",
        "        Returns the flattened feature vector: board state + one-hot encoded pieces.\n",
        "        \"\"\"\n",
        "        field_array = self.tetris.field_array\n",
        "        features = self._extract_features(field_array)\n",
        "\n",
        "        # One-hot encode current piece\n",
        "        current_piece = np.zeros(7, dtype=np.float32)\n",
        "        shape_to_idx = {shape: i for i, shape in enumerate(['I', 'O', 'T', 'L', 'J', 'S', 'Z'])}\n",
        "        if self.tetris.tetromino and hasattr(self.tetris.tetromino, 'shape'):\n",
        "            current_piece[shape_to_idx[self.tetris.tetromino.shape]] = 1.0\n",
        "\n",
        "        # One-hot encode next piece\n",
        "        next_piece = np.zeros(7, dtype=np.float32)\n",
        "        if self.tetris.next_tetromino and hasattr(self.tetris.next_tetromino, 'shape'):\n",
        "            next_piece[shape_to_idx[self.tetris.next_tetromino.shape]] = 1.0\n",
        "\n",
        "        # Final state = features + current_piece + next_piece\n",
        "        full_state = np.concatenate([features, current_piece, next_piece]).astype(np.float32)\n",
        "        return full_state, {}, {}\n",
        "\n",
        "    def _extract_features(self, field_array):\n",
        "        \"\"\"\n",
        "        Extracts numerical features from the board for learning:\n",
        "        height, holes, bumpiness, transitions, wells, etc.\n",
        "        \"\"\"\n",
        "        heights = self._get_heights()\n",
        "        max_height = max(heights) if heights else 0\n",
        "        holes = self._count_holes(heights)\n",
        "        bumpiness = self._calculate_bumpiness(heights)\n",
        "\n",
        "        # Calculate wells between columns\n",
        "        wells = []\n",
        "        for x in range(self.field_width):\n",
        "            l = heights[x-1] if x > 0 else self.field_height\n",
        "            r = heights[x+1] if x < self.field_width-1 else self.field_height\n",
        "            wells.append(max(0, min(l, r) - heights[x]))\n",
        "\n",
        "        # Count transitions between filled/empty cells in rows and columns\n",
        "        row_trans = col_trans = 0\n",
        "        for y in range(self.field_height):\n",
        "            filled = 1\n",
        "            for x in range(self.field_width):\n",
        "                cell = 1 if field_array[y][x] else 0\n",
        "                if cell != filled:\n",
        "                    row_trans += 1\n",
        "                filled = cell\n",
        "            if filled == 0:\n",
        "                row_trans += 1\n",
        "        for x in range(self.field_width):\n",
        "            filled = 1\n",
        "            for y in range(self.field_height):\n",
        "                cell = 1 if field_array[y][x] else 0\n",
        "                if cell != filled:\n",
        "                    col_trans += 1\n",
        "                filled = cell\n",
        "            if filled == 0:\n",
        "                col_trans += 1\n",
        "\n",
        "        cum_wells = sum(wells)\n",
        "        eroded_cells = self.tetris.lines_last_step * 10\n",
        "        diffs = [heights[i] - heights[i+1] for i in range(len(heights)-1)]\n",
        "\n",
        "        # Final feature vector of 50 dimensions\n",
        "        features = np.array(\n",
        "            heights + [max_height, holes, bumpiness] + wells + diffs +\n",
        "            [row_trans, col_trans, cum_wells, eroded_cells],\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        return features\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Executes the selected action and returns:\n",
        "        next_state, reward, done, truncated=False, info.\n",
        "        \"\"\"\n",
        "        if self.game_over:\n",
        "            return self._get_state(), 0.0, True, False, {}\n",
        "\n",
        "        column, rotation = self.action_space[action]\n",
        "        self._execute_action(column, rotation)\n",
        "        reward, lines_cleared = self._calculate_reward()\n",
        "\n",
        "        self.lines_cleared += lines_cleared\n",
        "        self.pieces_placed += 1\n",
        "        self.score = self.tetris.score\n",
        "        self.game_over = self.tetris.is_game_over()\n",
        "\n",
        "        observation = self._get_state()\n",
        "        info = {\n",
        "            'lines_cleared': lines_cleared,\n",
        "            'total_lines': self.lines_cleared,\n",
        "            'pieces_placed': self.pieces_placed,\n",
        "            'score': self.score\n",
        "        }\n",
        "        return observation, reward, self.game_over, False, info\n",
        "\n",
        "    def _execute_action(self, column, rotation):\n",
        "        \"\"\"\n",
        "        Rotates and moves the tetromino to the desired location, then hard-drops it.\n",
        "        \"\"\"\n",
        "        # Apply rotation\n",
        "        for _ in range(rotation):\n",
        "            self.tetris.tetromino.rotate()\n",
        "\n",
        "        # Move to desired column\n",
        "        min_x = min(int(block.pos.x) for block in self.tetris.tetromino.blocks)\n",
        "        move = int(column - min_x)\n",
        "        direction = 'right' if move > 0 else 'left'\n",
        "        for _ in range(abs(move)):\n",
        "            self.tetris.tetromino.move(direction=direction)\n",
        "\n",
        "        # Hard drop\n",
        "        while not self.tetris.tetromino.check_landing():\n",
        "            self.tetris.tetromino.move(direction='down')\n",
        "\n",
        "        self.tetris.check_tetromino_landing()\n",
        "\n",
        "    def _phi_state(self, heights, holes, bumpiness, lines):\n",
        "        \"\"\"Computes Dellacherie potential Φ(s) for reward shaping.\"\"\"\n",
        "        agg_height = sum(heights)\n",
        "        feats = np.array([agg_height, lines, holes, bumpiness], dtype=np.float32)\n",
        "        return float(np.dot(self.DELLACHERIE_W, feats))\n",
        "\n",
        "    def _calculate_reward(self):\n",
        "        \"\"\"Combines line reward, shaping, survival and penalties into final reward.\"\"\"\n",
        "        lines_cleared = self.tetris.lines_last_step\n",
        "        line_reward = {0: 0, 1: 40, 2: 100, 3: 300, 4: 1200}[lines_cleared]\n",
        "\n",
        "        heights = self._get_heights()\n",
        "        holes = self._count_holes(heights)\n",
        "        bumpiness = self._calculate_bumpiness(heights)\n",
        "\n",
        "        potential = self._phi_state(heights, holes, bumpiness, lines_cleared)\n",
        "        shaping = self.gamma * potential - self.prev_potential\n",
        "        self.prev_potential = potential\n",
        "\n",
        "        combo_bonus = min(self.tetris.combo_count * 0.5, 2.0) if lines_cleared else 0\n",
        "        board_penalty, bonus_near_full = self._evaluate_board_state()\n",
        "        survival_reward = 0.01\n",
        "        game_over_penalty = -5.0 if self.tetris.is_game_over() else 0\n",
        "\n",
        "        total_reward = (\n",
        "            line_reward * 2 + combo_bonus + bonus_near_full -\n",
        "            board_penalty + survival_reward + game_over_penalty +\n",
        "            shaping\n",
        "        )\n",
        "        return total_reward, lines_cleared\n",
        "\n",
        "    def _evaluate_board_state(self):\n",
        "        \"\"\"Evaluates penalties for bad board states and bonuses for near-complete rows.\"\"\"\n",
        "        penalty = 0.0\n",
        "        bonus = 0.0\n",
        "\n",
        "        heights = self._get_heights()\n",
        "        max_height = max(heights) if heights else 0\n",
        "        height_penalty = max(0, max_height - self.field_height / 2) * 0.02\n",
        "\n",
        "        holes = self._count_holes(heights)\n",
        "        holes_penalty = holes * 0.05\n",
        "\n",
        "        # Penalize deep holes\n",
        "        deep_holes_penalty = 0.0\n",
        "        for x in range(self.field_width):\n",
        "            column_hole_depth = 0\n",
        "            seen_block = False\n",
        "            for y in range(self.field_height):\n",
        "                if self.tetris.field_array[y][x]:\n",
        "                    seen_block = True\n",
        "                elif seen_block:\n",
        "                    column_hole_depth += 1\n",
        "            deep_holes_penalty += (column_hole_depth ** 2) * 0.001\n",
        "\n",
        "        bumpiness = self._calculate_bumpiness(heights)\n",
        "        bumpiness_penalty = bumpiness * 0.01\n",
        "\n",
        "        for y in range(self.field_height):\n",
        "            filled = sum(1 for x in range(self.field_width) if self.tetris.field_array[y][x])\n",
        "            if filled == self.field_width - 1:\n",
        "                bonus += 0.5\n",
        "\n",
        "        penalty = height_penalty + holes_penalty + bumpiness_penalty + deep_holes_penalty\n",
        "        return penalty, bonus\n",
        "\n",
        "    def _get_heights(self):\n",
        "        \"\"\"Returns the height of each column based on filled cells.\"\"\"\n",
        "        heights = []\n",
        "        for x in range(self.field_width):\n",
        "            for y in range(self.field_height):\n",
        "                if self.tetris.field_array[y][x]:\n",
        "                    heights.append(self.field_height - y)\n",
        "                    break\n",
        "            else:\n",
        "                heights.append(0)\n",
        "        return heights\n",
        "\n",
        "    def _count_holes(self, heights):\n",
        "        \"\"\"Counts the number of empty cells below filled cells in each column.\"\"\"\n",
        "        holes = 0\n",
        "        for x in range(self.field_width):\n",
        "            top = self.field_height - heights[x] if x < len(heights) else 0\n",
        "            for y in range(top + 1, self.field_height):\n",
        "                if not self.tetris.field_array[y][x]:\n",
        "                    holes += 1\n",
        "        return holes\n",
        "\n",
        "    def _calculate_bumpiness(self, heights):\n",
        "        \"\"\"Returns the sum of absolute differences between adjacent column heights.\"\"\"\n",
        "        if not heights or len(heights) < 2:\n",
        "            return 0\n",
        "        return sum(abs(heights[i] - heights[i+1]) for i in range(len(heights) - 1))\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Renders the game window if in human mode.\"\"\"\n",
        "        if self.has_display:\n",
        "            self.app.draw()\n",
        "            pg.display.flip()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Shuts down the game and closes the display.\"\"\"\n",
        "        if self.has_display:\n",
        "            pg.quit()\n",
        "\n",
        "    def render_board_as_image(self):\n",
        "        \"\"\"\n",
        "        Converts the board into a color-coded NumPy RGB image for visualization or recording.\n",
        "        \"\"\"\n",
        "        shape_colors = {\n",
        "            'I': (0, 255, 255), 'O': (255, 255, 0), 'T': (128, 0, 128),\n",
        "            'S': (0, 255, 0), 'Z': (255, 0, 0), 'J': (0, 0, 255),\n",
        "            'L': (255, 165, 0), 'default': (200, 200, 200)\n",
        "        }\n",
        "\n",
        "        board = np.zeros((self.field_height, self.field_width, 3), dtype=np.uint8)\n",
        "\n",
        "        for y in range(self.field_height):\n",
        "            for x in range(self.field_width):\n",
        "                cell = self.tetris.field_array[y][x]\n",
        "                if cell:\n",
        "                    shape = getattr(cell.tetromino, 'shape', 'default') if hasattr(cell, 'tetromino') else 'default'\n",
        "                    board[y, x] = shape_colors.get(shape, shape_colors['default'])\n",
        "\n",
        "        return board"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZJblsH3gllY"
      },
      "source": [
        "<div dir = rtl>\n",
        "\n",
        "# (4) ארכיטקטורת הרשת"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir = rtl>\n",
        "\n",
        "\n",
        "המודל TetrisDQN הוא רשת עצבית עבור סוכן למידת חיזוק מסוג Dueling DQN, שמתאימה במיוחד למשחקים כמו טטריס. הרשת מחלקת את החיזוי לשני זרמים:\n",
        "\n",
        "Value stream שמעריך את ערך המצב (V).\n",
        "\n",
        "Advantage stream שמעריך את היתרון של כל פעולה (A) ביחס לממוצע.\n",
        "שני הזרמים מחוברים לפי הנוסחה Q(s,a) = V(s) + (A(s,a) - mean(A(s,·))), כך שהרשת לומדת טוב יותר במצבים שבהם לפעולות שונות יש ערכים דומים."
      ],
      "metadata": {
        "id": "xOT0K2hz8rWI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1UnS0r9T8WPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b55b61-6747-447e-af13-db7de29b73cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TetrisDQN(\n",
            "  (feature): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=128, bias=True)\n",
            "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (value_stream): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
            "  )\n",
            "  (adv_stream): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=32, out_features=20, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class TetrisDQN(nn.Module):\n",
        "    def __init__(self, input_size, n_actions):\n",
        "        super().__init__()\n",
        "\n",
        "        # Shared feature extraction layers (common to both value and advantage streams)\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),     # Fully connected layer from input to 128 features\n",
        "            nn.LayerNorm(128),              # Normalize layer output to stabilize training\n",
        "            nn.ReLU(),                      # Non-linearity\n",
        "            nn.Linear(128, 64),             # Second dense layer reduces to 64 features\n",
        "            nn.LayerNorm(64),               # Normalize again\n",
        "            nn.ReLU()                       # Activation\n",
        "        )\n",
        "\n",
        "        # Value stream: outputs a single value V(s) for the given state\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(64, 32),              # Compress features\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)                # Output scalar state value\n",
        "        )\n",
        "\n",
        "        # Advantage stream: outputs A(s,a) for each action\n",
        "        self.adv_stream = nn.Sequential(\n",
        "            nn.Linear(64, 32),              # Compress features\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, n_actions)        # Output one advantage per action\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature(x)                 # Extract features from input\n",
        "        v = self.value_stream(x)           # Compute V(s)\n",
        "        a = self.adv_stream(x)             # Compute A(s,a)\n",
        "        return v + (a - a.mean(dim=1, keepdim=True))  # Combine into Q(s,a) using dueling formula\n",
        "\n",
        "# Create and print model architecture\n",
        "model = TetrisDQN(input_size=50, n_actions=20)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-JSpUvpg2-6"
      },
      "source": [
        "<div dir = rtl>\n",
        "\n",
        "# (5) מאגר חוויות"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir = rtl>\n",
        "\n",
        "המחלקה PrioritizedReplayBuffer מממשת מאגר חוויות חכם ללמידת חיזוק עם דגימה מועדפת (PER). במקום לבחור חוויות באקראי, המאגר הזה נותן עדיפות לחוויות עם שגיאת חיזוי גבוהה – כלומר, כאלה שמהן אפשר ללמוד יותר.\n",
        "הפרמטר α שולט כמה חזקה ההעדפה, ו־β מתקן את ההטיה שנוצרת עקב כך באמצעות משקולות. זה מאפשר לשפר את יעילות האימון ולצמצם בזבוז על חוויות פחות חשובות."
      ],
      "metadata": {
        "id": "9YgCiVVx_VCU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Dh436t0LhbFo"
      },
      "outputs": [],
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "    \"\"\"\n",
        "    A memory buffer implementing Prioritized Experience Replay (PER).\n",
        "    It samples more important transitions (with higher TD-error).\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_end=1.0, beta_frames=100000):\n",
        "        \"\"\"\n",
        "        Initialize the buffer.\n",
        "\n",
        "        Parameters:\n",
        "        - capacity: maximum number of stored experiences\n",
        "        - alpha: prioritization exponent (0 = uniform, 1 = full prioritization)\n",
        "        - beta_start: initial value of beta for importance-sampling correction\n",
        "        - beta_end: final value of beta\n",
        "        - beta_frames: number of frames over which beta is annealed\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.priorities = np.zeros((capacity,), dtype=np.float32)  # priority array\n",
        "        self.position = 0  # index for overwriting old entries\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "        self.beta_frames = beta_frames\n",
        "        self.beta = beta_start\n",
        "        self.frame = 0\n",
        "\n",
        "        self.Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
        "\n",
        "    def update_beta(self, frame=None):\n",
        "        \"\"\"\n",
        "        Update beta based on frame count for importance-sampling weight correction.\n",
        "        Returns updated beta.\n",
        "        \"\"\"\n",
        "        if frame is None:\n",
        "            self.frame += 1\n",
        "            frame = self.frame\n",
        "\n",
        "        self.beta = min(\n",
        "            self.beta_end,\n",
        "            self.beta_start + (self.beta_end - self.beta_start) * (frame / self.beta_frames)\n",
        "        )\n",
        "        return self.beta\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Add a new experience to the buffer.\n",
        "        New samples are given the maximum priority to ensure they're sampled at least once.\n",
        "        \"\"\"\n",
        "        max_priority = self.priorities.max() if self.memory else 1.0\n",
        "\n",
        "        # Add or overwrite the experience\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(self.Experience(state, action, reward, next_state, done))\n",
        "        else:\n",
        "            self.memory[self.position] = self.Experience(state, action, reward, next_state, done)\n",
        "\n",
        "        # Assign max priority to the new experience\n",
        "        self.priorities[self.position] = max_priority\n",
        "        self.position = (self.position + 1) % self.capacity  # cyclic buffer\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample a batch of experiences based on priorities.\n",
        "\n",
        "        Returns:\n",
        "        - batch: named tuple with arrays of (states, actions, etc.)\n",
        "        - indices: selected sample indices (used for updating priorities)\n",
        "        - weights: importance-sampling weights to correct the bias\n",
        "        \"\"\"\n",
        "        if len(self.memory) == self.capacity:\n",
        "            priorities = self.priorities\n",
        "        else:\n",
        "            priorities = self.priorities[:self.position]\n",
        "\n",
        "        # Compute sampling probabilities\n",
        "        probs = priorities ** self.alpha\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        # Sample indices according to probabilities\n",
        "        indices = np.random.choice(len(self.memory), batch_size, p=probs)\n",
        "\n",
        "        # Compute importance-sampling weights\n",
        "        weights = (len(self.memory) * probs[indices]) ** (-self.beta)\n",
        "        weights /= weights.max()  # normalize\n",
        "        weights = np.array(weights, dtype=np.float32)\n",
        "\n",
        "        # Build batch from sampled indices\n",
        "        batch = self.Experience(\n",
        "            state=[self.memory[idx].state for idx in indices],\n",
        "            action=[self.memory[idx].action for idx in indices],\n",
        "            reward=[self.memory[idx].reward for idx in indices],\n",
        "            next_state=[self.memory[idx].next_state for idx in indices],\n",
        "            done=[self.memory[idx].done for idx in indices]\n",
        "        )\n",
        "\n",
        "        return batch, indices, weights\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        \"\"\"\n",
        "        Update the priorities of sampled experiences after learning step.\n",
        "        \"\"\"\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.priorities[idx] = priority\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of the buffer.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7kP7fwRi_rM"
      },
      "source": [
        "<div dir = rtl>\n",
        "\n",
        "#(6) בניית הסוכן"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir = rtl>\n",
        "\n",
        "המחלקה DQNAgent מגדירה סוכן למידת חיזוק מבוסס DQN עם תוספות מתקדמות:\n",
        "\n",
        "Double DQN להפחתת הטיית הערכת הפעולה הטובה ביותר.\n",
        "\n",
        "Prioritized Experience Replay (PER) שמעדיף חוויות עם שגיאת חיזוי גבוהה כדי לייעל את הלמידה.\n",
        "\n",
        "AMP (חישוב מדויק למחצה) להאצת האימון על GPU.\n",
        "\n",
        "Epsilon Decay לשליטה חכמה באקראיות לאורך זמן – מתחילים בחקירה (ε=1) ומתקדמים לניצול.\n",
        "\n",
        "שמירה וטעינה של מודל כולל משקלים, optimizer ומספר צעדים.\n",
        "\n",
        "לוגים ל-TensorBoard למעקב אחרי האימון.\n",
        "\n",
        "הסוכן מבצע בחירה של פעולה לפי ε-greedy, שומר חוויות בזיכרון מתעדף, ומאמן את הרשת באצוות תוך שימוש ב־TD-error וחיזוי עתידי עם הרשת היעד."
      ],
      "metadata": {
        "id": "m5VNfpKLAao4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5QveBzYF6ERp"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    DQN Agent with Epsilon-Greedy strategy, Double DQN, PER, AMP, and LR scheduler.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, n_actions, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        \"\"\"\n",
        "        Initialize the agent with two networks, replay buffer, optimizer, and training parameters.\n",
        "\n",
        "        Parameters:\n",
        "        - input_size: flattened state vector size\n",
        "        - n_actions: number of possible discrete actions\n",
        "        - device: computation device (CPU or CUDA)\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.n_actions = n_actions\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize online (policy) and target networks\n",
        "        self.policy_net = TetrisDQN(input_size, n_actions).to(device)\n",
        "        self.target_net = TetrisDQN(input_size, n_actions).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())  # sync weights\n",
        "        self.target_net.eval()  # target net is not trained\n",
        "\n",
        "        # Prioritized replay buffer\n",
        "        self.memory = PrioritizedReplayBuffer(capacity=100_000)\n",
        "\n",
        "        # Optimizer and learning rate schedule\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=5e-4)\n",
        "        self.lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=1e6)\n",
        "\n",
        "        # Epsilon decay schedule\n",
        "        self.eps_start = 1.0\n",
        "        self.eps_end = 0.05\n",
        "        self.eps_decay_steps = 425_000\n",
        "        self.steps_done = 0\n",
        "\n",
        "        # Gradient scaler for AMP (mixed precision)\n",
        "        self.scaler = torch.cuda.amp.GradScaler(enabled=(self.device == 'cuda'))\n",
        "\n",
        "        # Discount factor and update frequencies\n",
        "        self.gamma = 0.999\n",
        "        self.target_update = 1000\n",
        "        self.batch_size = 128\n",
        "\n",
        "        # TensorBoard writer for logging\n",
        "        self.writer = SummaryWriter(log_dir='runs/tetris_dqn')\n",
        "\n",
        "    def _get_epsilon(self):\n",
        "        \"\"\"\n",
        "        Compute current epsilon value based on decay schedule.\n",
        "        \"\"\"\n",
        "        frac = min(1.0, self.steps_done / self.eps_decay_steps)\n",
        "        epsilon = self.eps_end + (self.eps_start - self.eps_end) * (1 - frac)\n",
        "        return epsilon\n",
        "\n",
        "    def preprocess_state(self, state):\n",
        "        \"\"\"\n",
        "        Converts raw state into a FloatTensor for model input.\n",
        "\n",
        "        Handles tuple wrapping, numpy arrays, lists, etc.\n",
        "        \"\"\"\n",
        "        if isinstance(state, (tuple, list)) and len(state) > 0:\n",
        "            if isinstance(state[0], (list, np.ndarray, float, int)):\n",
        "                state = state[0]\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        return state_tensor\n",
        "\n",
        "    def select_action(self, state, eval_mode=False):\n",
        "        \"\"\"\n",
        "        Select an action using epsilon-greedy policy.\n",
        "\n",
        "        Parameters:\n",
        "        - state: current state\n",
        "        - eval_mode: if True, disables exploration\n",
        "\n",
        "        Returns:\n",
        "        - selected action (int)\n",
        "        \"\"\"\n",
        "        if eval_mode:\n",
        "            epsilon = 0.0\n",
        "        else:\n",
        "            epsilon = self._get_epsilon()\n",
        "            self.steps_done += 1\n",
        "\n",
        "        if random.random() < epsilon:\n",
        "            return random.randrange(self.n_actions)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.policy_net.eval()\n",
        "            state_tensor = self.preprocess_state(state)\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            self.policy_net.train()\n",
        "            return q_values.max(1)[1].item()\n",
        "\n",
        "    def optimize_model(self):\n",
        "        \"\"\"\n",
        "        Perform one optimization step using Double DQN, PER, AMP, and gradient clipping.\n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample batch from replay buffer\n",
        "        batch, indices, weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        # Preprocess states and actions\n",
        "        state_batch = torch.cat([self.preprocess_state(s) for s in batch.state], dim=0)\n",
        "        next_state_batch = torch.cat([self.preprocess_state(s) for s in batch.next_state], dim=0)\n",
        "        action_batch = torch.tensor(batch.action, device=self.device).unsqueeze(1)\n",
        "        reward_batch = torch.tensor(batch.reward, device=self.device)\n",
        "        done_batch = torch.tensor(batch.done, dtype=torch.float32, device=self.device)\n",
        "        weights_tensor = torch.FloatTensor(weights).to(self.device)\n",
        "\n",
        "        self.policy_net.train()\n",
        "\n",
        "        # Automatic Mixed Precision (faster on GPU)\n",
        "        with torch.cuda.amp.autocast(enabled=(self.device == 'cuda')):\n",
        "            # Compute Q(s,a)\n",
        "            q_values = self.policy_net(state_batch).gather(1, action_batch).squeeze(1)\n",
        "\n",
        "            # Compute target Q-values using Double DQN\n",
        "            with torch.no_grad():\n",
        "                next_actions = self.policy_net(next_state_batch).max(1)[1].unsqueeze(1)\n",
        "                next_q_values = self.target_net(next_state_batch).gather(1, next_actions).squeeze(1)\n",
        "                expected_q_values = reward_batch + self.gamma * next_q_values * (1 - done_batch)\n",
        "\n",
        "            # Compute TD error and update priorities in PER\n",
        "            td_errors = torch.abs(expected_q_values - q_values).detach().cpu().numpy()\n",
        "            self.memory.update_priorities(indices, td_errors + 1e-5)\n",
        "\n",
        "            # Compute loss using weighted Smooth L1 loss (Huber)\n",
        "            loss = F.smooth_l1_loss(q_values, expected_q_values, reduction='none')\n",
        "            loss = (loss * weights_tensor).mean()\n",
        "\n",
        "        # Gradient step with AMP\n",
        "        self.optimizer.zero_grad()\n",
        "        self.scaler.scale(loss).backward()\n",
        "        self.scaler.unscale_(self.optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.scaler.update()\n",
        "\n",
        "        # Adjust learning rate\n",
        "        self.lr_sched.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"\n",
        "        Save model weights and optimizer state to file.\n",
        "        \"\"\"\n",
        "        torch.save({\n",
        "            'policy_net_state_dict': self.policy_net.state_dict(),\n",
        "            'target_net_state_dict': self.target_net.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'steps_done': self.steps_done\n",
        "        }, path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        \"\"\"\n",
        "        Load model and optimizer state from file.\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(path)\n",
        "        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
        "        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.steps_done = checkpoint['steps_done']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dpK8WXsjRyP"
      },
      "source": [
        "<div dir = rtl>\n",
        "\n",
        "# (7) תהליך האימון"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir = rtl>\n",
        "\n",
        "evaluate_agent\n",
        "פונקציה זו אחראית להעריך את ביצועי הסוכן. היא מריצה את הסוכן במספר אפיזודות (ברירת מחדל: 10) ובודקת:\n",
        "\n",
        "כמה תגמול הוא השיג (reward)\n",
        "\n",
        "כמה שורות הוא שבר (lines)\n",
        "\n",
        "כמה צעדים נדרשו (steps)\n",
        "\n",
        "היא תומכת בהדמיה גרפית (render=True) וכוללת מנגנון הדפסה מפורט (verbose=True).\n",
        "התוצאה היא ממוצע של ביצועי הסוכן בפרמטרים השונים — ומספקת מדד להשוואה לאורך זמן.\n",
        "\n",
        "train_agent\n",
        "פונקציה זו מבצעת את אימון הסוכן בפועל:\n",
        "\n",
        "כל אפיזודה מתחילה מאתחול הלוח ומסתיימת כשיש Game Over או שמגיעים למקסימום צעדים.\n",
        "\n",
        "בכל צעד: הסוכן בוחר פעולה (עם ε-greedy), מבצע אותה, שומר את החוויה ב־PER buffer, ואז מתעדכן.\n",
        "\n",
        "אחת לכמה אפיזודות מתבצעת הערכה (ללא חקירה) כדי לבדוק את ההתקדמות.\n",
        "\n",
        "המודל נשמר כשמתקבל שיא חדש, והמודל הסופי נשמר בסיום.\n",
        "\n",
        "בנוסף, נשמרים נתוני לוג רבים: תגמולים, שורות שנשברו, אורך אפיזודות, שגיאת למידה (loss), ערכי ε, בטחון בפעולות (softmax), ועוד.\n",
        "\n",
        "staged_training\n",
        "זוהי פונקציה לתמיכה ב־למידה בשלבים (Curriculum Learning). כל שלב כולל:\n",
        "\n",
        "גודל לוח שונה וצורות שונות.\n",
        "\n",
        "אימון נפרד עם קונפיגורציה שונה.\n",
        "\n",
        "אפשרות ל־Transfer Learning — העברת משקלים בין שלבים.\n",
        "\n",
        "הפונקציה בונה את הסוכן מחדש בכל שלב, ומבצעת בדיקה האם ניתן להעביר את המשקלים (בהתאם לגודל הקלט).\n",
        "לאחר כל שלב מתבצעת הערכה סופית וניתוח בחירות הסוכן (אם יש analyze_agent_decisions).\n",
        "בסיום מתקבלות שתי תוצאות:\n",
        "\n",
        "מילון של כל הסוכנים המאומנים לפי שלב\n",
        "\n",
        "לוג מפורט של תוצאות האימון לכל שלב"
      ],
      "metadata": {
        "id": "n0gJmCYdC1pq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PHyZAA3QFf8K"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, agent, num_episodes=10, render=False, verbose=False):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of the agent across several episodes.\n",
        "\n",
        "    Args:\n",
        "        env: The environment to run evaluation in.\n",
        "        agent: The DQN agent to be evaluated.\n",
        "        num_episodes: Number of episodes to evaluate over.\n",
        "        render: Whether to visually render the environment during evaluation.\n",
        "        verbose: Whether to print detailed per-episode results.\n",
        "\n",
        "    Returns:\n",
        "        avg_reward: Average reward across all evaluation episodes.\n",
        "    \"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        # Display detailed header if verbosity is enabled\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Evaluating agent over {num_episodes} episodes...\")\n",
        "        print(f\"{'='*50}\")\n",
        "    else:\n",
        "        # Minimal output if not verbose\n",
        "        print(f\"\\nEvaluating agent... \", end='', flush=True)\n",
        "\n",
        "    # Track metrics for each episode\n",
        "    rewards = []\n",
        "    lines_cleared_list = []\n",
        "    steps_list = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        state, _ = env.reset()               # Reset the environment\n",
        "        episode_reward = 0                   # Initialize reward counter\n",
        "        done = False                         # Episode end flag\n",
        "        steps = 0                            # Step counter\n",
        "        lines_cleared = 0                    # Line clear counter\n",
        "        max_steps = 10000                    # Safety limit to prevent infinite loops\n",
        "\n",
        "        while not done and steps < max_steps:\n",
        "            # Select greedy action (no randomness)\n",
        "            action = agent.select_action(state, eval_mode=True)\n",
        "\n",
        "            try:\n",
        "                # Apply action in environment\n",
        "                next_state, reward, terminated, truncated, info = env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                # Update line clear count from info dict\n",
        "                lines_cleared += info.get('lines_cleared', 0)\n",
        "\n",
        "            except Exception as e:\n",
        "                # Handle errors gracefully\n",
        "                print(f\"Exception during step: {e}\")\n",
        "                break\n",
        "\n",
        "            # Update state and accumulate reward\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "            # Render the environment if requested\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "        # Store episode results\n",
        "        rewards.append(episode_reward)\n",
        "        lines_cleared_list.append(lines_cleared)\n",
        "        steps_list.append(steps)\n",
        "\n",
        "        if verbose:\n",
        "            # Print episode-specific stats\n",
        "            print(f\"  Episode {ep+1}/{num_episodes}: reward = {episode_reward:.2f}, steps = {steps}, lines = {lines_cleared}\")\n",
        "\n",
        "    # Compute overall evaluation statistics\n",
        "    avg_reward = np.mean(rewards)\n",
        "    avg_steps = np.mean(steps_list)\n",
        "    avg_lines = np.mean(lines_cleared_list)\n",
        "\n",
        "    if verbose:\n",
        "        # Show full summary\n",
        "        print(f\"\\nEvaluation results:\")\n",
        "        print(f\"  Average reward: {avg_reward:.2f}\")\n",
        "        print(f\"  Average steps: {avg_steps:.1f}\")\n",
        "        print(f\"  Average lines cleared: {avg_lines:.1f}\")\n",
        "        print(f\"{'='*50}\")\n",
        "    else:\n",
        "        # Concise summary\n",
        "        print(f\"done. Avg reward: {avg_reward:.2f}, Avg lines: {avg_lines:.1f}\")\n",
        "\n",
        "    return avg_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "h_MkvuhfjjhW"
      },
      "outputs": [],
      "source": [
        "def train_agent(env, agent, num_episodes=10000, max_steps_per_episode=10000,\n",
        "               eval_freq=100, eval_episodes=10, save_freq=500, model_dir='models', stage=None):\n",
        "    \"\"\"\n",
        "    Train a DQN agent in the given environment with logging, evaluation, and saving.\n",
        "\n",
        "    Args:\n",
        "        env: The Tetris environment.\n",
        "        agent: The DQNAgent to train.\n",
        "        num_episodes: Total training episodes.\n",
        "        max_steps_per_episode: Max steps allowed per episode.\n",
        "        eval_freq: How often to evaluate the agent (in episodes).\n",
        "        eval_episodes: How many episodes to average during evaluation.\n",
        "        save_freq: Frequency of printing board state.\n",
        "        model_dir: Directory to save models.\n",
        "        stage: Optional training stage identifier (for logging).\n",
        "    \"\"\"\n",
        "    # Ensure model directory exists\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Logging and metrics storage\n",
        "    all_rewards = []\n",
        "    all_lengths = []\n",
        "    all_losses = []\n",
        "    all_lines_cleared = []\n",
        "    eval_rewards = []\n",
        "    best_eval_reward = float('-inf')\n",
        "\n",
        "    # Extra logs\n",
        "    epsilon_values = []\n",
        "    lines_cleared_per_episode = []\n",
        "    learning_rates = []\n",
        "    action_counts = np.zeros((10, 4), dtype=int)  # count per (column, rotation)\n",
        "    avg_action_confidences = []\n",
        "    board_fill_counts = np.zeros((env.field_height, env.field_width), dtype=int)\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Starting training for {num_episodes} episodes\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    pbar = tqdm(range(num_episodes), desc=\"Training\")\n",
        "\n",
        "    for episode in pbar:\n",
        "        epsilon_values.append(agent._get_epsilon())\n",
        "        current_lr = agent.optimizer.param_groups[0]['lr']\n",
        "        learning_rates.append(current_lr)\n",
        "\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_loss = 0\n",
        "        num_optimization_steps = 0\n",
        "        lines_cleared = 0\n",
        "        pieces_placed = 0\n",
        "        confidences = []\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            # Choose action and log confidence\n",
        "            action = agent.select_action(state)\n",
        "            state_tensor = agent.preprocess_state(state)\n",
        "            with torch.no_grad():\n",
        "                q_vals = agent.policy_net(state_tensor)\n",
        "                action_confidence = torch.softmax(q_vals, dim=1).max().item()\n",
        "                confidences.append(action_confidence)\n",
        "\n",
        "            # Count action statistics\n",
        "            col, rot = env.action_space[action]\n",
        "            action_counts[col][rot] += 1\n",
        "\n",
        "            # Take action\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Update board fill count if game over\n",
        "            if done:\n",
        "                field = env.tetris.field_array\n",
        "                for y in range(env.field_height):\n",
        "                    for x in range(env.field_width):\n",
        "                        if field[y][x]:\n",
        "                            board_fill_counts[y][x] += 1\n",
        "\n",
        "            # Track environment metrics\n",
        "            if 'lines_cleared' in info:\n",
        "                lines_cleared += info['lines_cleared']\n",
        "            if 'pieces_placed' in info:\n",
        "                pieces_placed = info.get('pieces_placed', step + 1)\n",
        "\n",
        "            # Store experience and train\n",
        "            agent.memory.push(state, action, reward, next_state, done)\n",
        "            loss = agent.optimize_model()\n",
        "            if loss is not None:\n",
        "                episode_loss += loss\n",
        "                num_optimization_steps += 1\n",
        "\n",
        "            # Update importance-sampling beta for PER\n",
        "            agent.memory.update_beta()\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Compute episode statistics\n",
        "        avg_loss = episode_loss / max(1, num_optimization_steps)\n",
        "        avg_conf = np.mean(confidences) if confidences else 0\n",
        "\n",
        "        # Log results\n",
        "        all_rewards.append(episode_reward)\n",
        "        all_lengths.append(step + 1)\n",
        "        all_losses.append(avg_loss)\n",
        "        all_lines_cleared.append(lines_cleared)\n",
        "        lines_cleared_per_episode.append(lines_cleared)\n",
        "        avg_action_confidences.append(avg_conf)\n",
        "\n",
        "        # Periodically print board visualization\n",
        "        if (episode + 1) % save_freq == 0:\n",
        "            pbar.clear()\n",
        "            print_board_state(env, episode + 1)\n",
        "\n",
        "        # Compute averages for display\n",
        "        avg_reward_last_100 = np.mean(all_rewards[-100:]) if len(all_rewards) >= 100 else np.mean(all_rewards)\n",
        "        avg_lines_last_100 = np.mean(all_lines_cleared[-100:]) if len(all_lines_cleared) >= 100 else np.mean(all_lines_cleared)\n",
        "        best_reward = max(all_rewards) if all_rewards else 0\n",
        "\n",
        "        # Update tqdm display\n",
        "        pbar.set_postfix({\n",
        "            'Reward': f\"{episode_reward:.1f}\",\n",
        "            'avg100': f\"{avg_reward_last_100:.1f}\",\n",
        "            'eps': f\"{agent._get_epsilon():.2f}\",\n",
        "            'lines': f\"{lines_cleared}\",\n",
        "            'buffer': f\"{len(agent.memory)/agent.memory.capacity:.0%}\"\n",
        "        })\n",
        "\n",
        "        # TensorBoard logging\n",
        "        agent.writer.add_scalar('Training/Reward', episode_reward, episode)\n",
        "        agent.writer.add_scalar('Training/Length', step + 1, episode)\n",
        "        agent.writer.add_scalar('Training/Loss', avg_loss, episode)\n",
        "        agent.writer.add_scalar('Training/Epsilon', agent._get_epsilon(), episode)\n",
        "        agent.writer.add_scalar('Training/LinesCleared', lines_cleared, episode)\n",
        "        agent.writer.add_scalar('Training/BufferFill', len(agent.memory)/agent.memory.capacity, episode)\n",
        "\n",
        "        # Evaluation\n",
        "        if episode % eval_freq == 0:\n",
        "            eval_reward = evaluate_agent(env, agent, num_episodes=eval_episodes, verbose=False)\n",
        "            eval_rewards.append(eval_reward)\n",
        "            agent.writer.add_scalar('Evaluation/Reward', eval_reward, episode)\n",
        "\n",
        "            # Save best model so far\n",
        "            if eval_reward > best_eval_reward:\n",
        "                best_eval_reward = eval_reward\n",
        "                agent.save_model(f\"{model_dir}/best_model.pt\")\n",
        "                print(f\"  New best model saved! Reward: {best_eval_reward:.2f}\")\n",
        "\n",
        "    # Final training summary\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training complete after {num_episodes} episodes\")\n",
        "    print(f\"  Best evaluation reward: {best_eval_reward:.2f}\")\n",
        "    print(f\"  Average lines cleared (last 100 episodes): {avg_lines_last_100:.2f}\")\n",
        "    print(f\"  Maximum episode reward: {best_reward:.2f}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Final board snapshot and model save\n",
        "    print_board_state(env, num_episodes)\n",
        "    agent.save_model(f\"{model_dir}/final_model.pt\")\n",
        "\n",
        "    # Plotting and diagnostics\n",
        "    plot_training_progress(\n",
        "        rewards=all_rewards,\n",
        "        lengths=all_lengths,\n",
        "        losses=all_losses,\n",
        "        eval_rewards=eval_rewards,\n",
        "        epsilon_values=epsilon_values,\n",
        "        lines_cleared=lines_cleared_per_episode,\n",
        "        learning_rates=learning_rates,\n",
        "        confidence_values=avg_action_confidences,\n",
        "        action_counts=action_counts,\n",
        "        cell_fill_counts=board_fill_counts,\n",
        "        stage=stage\n",
        "    )\n",
        "\n",
        "    # Return all logs for external analysis\n",
        "    return all_rewards, all_lengths, all_losses, eval_rewards, epsilon_values, lines_cleared_per_episode, learning_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oaCEaZNxuDhm"
      },
      "outputs": [],
      "source": [
        "def staged_training(stages=(1, 2, 3, 4, 5), episodes_per_stage=2000, transfer_learning=True,\n",
        "                    input_size=None, action_size=None):\n",
        "    \"\"\"\n",
        "    Train a DQN agent in multiple curriculum stages with optional transfer learning.\n",
        "\n",
        "    Args:\n",
        "        stages: List/tuple of stage numbers (each defines a different environment setup).\n",
        "        episodes_per_stage: Number of episodes to train per stage.\n",
        "        transfer_learning: Whether to transfer weights between stages.\n",
        "        input_size: Optional fixed input size (autodetected if None).\n",
        "        action_size: Optional fixed action space size.\n",
        "\n",
        "    Returns:\n",
        "        trained_agents: A dictionary of trained agents per stage.\n",
        "        training_metrics: A dictionary of training logs per stage.\n",
        "    \"\"\"\n",
        "\n",
        "    trained_agents = {}      # Store trained agents by stage\n",
        "    training_metrics = {}    # Store performance metrics\n",
        "\n",
        "    for i, stage in enumerate(stages):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"STAGE {stage} TRAINING\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Board size: {STAGE_BOARD_SIZES.get(stage, (10, 20))}\")\n",
        "        _, _, allowed_shapes = stage_params(stage)\n",
        "        print(f\"Allowed shapes: {allowed_shapes}\")\n",
        "        print(f\"Episodes: {episodes_per_stage}\")\n",
        "        print(f\"Transfer learning: {transfer_learning}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Create new environment for current stage\n",
        "        env = TetrisWrapper(render_mode=None, stage=stage)\n",
        "\n",
        "        # Detect input size and action space size\n",
        "        raw_state, _ = env.reset()\n",
        "        agent_temp = DQNAgent(input_size=1, n_actions=1)  # temporary dummy\n",
        "        state_tensor = agent_temp.preprocess_state(raw_state)\n",
        "        current_input_size = state_tensor.shape[1]\n",
        "        current_action_size = len(env.action_space)\n",
        "\n",
        "        input_size = current_input_size\n",
        "        action_size = action_size or current_action_size\n",
        "\n",
        "        # Initialize new agent\n",
        "        agent = DQNAgent(input_size=input_size, n_actions=action_size)\n",
        "\n",
        "        if i == 0 or not transfer_learning:\n",
        "            print(f\"Created new agent for stage {stage}\")\n",
        "        else:\n",
        "            # Attempt transfer learning from previous stage\n",
        "            prev_stage = stages[i - 1]\n",
        "            prev_agent = trained_agents[prev_stage]\n",
        "\n",
        "            # Only transfer if input size matches\n",
        "            if prev_agent.input_size == current_input_size:\n",
        "                agent.policy_net.load_state_dict(prev_agent.policy_net.state_dict())\n",
        "                agent.target_net.load_state_dict(prev_agent.target_net.state_dict())\n",
        "                print(f\"Transferred weights from stage {prev_stage} to stage {stage}\")\n",
        "            else:\n",
        "                print(f\"⚠️ Skipping transfer: input size mismatch ({prev_agent.input_size} → {current_input_size})\")\n",
        "\n",
        "        # Directory to save models for this stage\n",
        "        model_dir = f\"models/stage_{stage}\"\n",
        "\n",
        "        # Train agent on current stage\n",
        "        all_rewards, all_lengths, all_losses, eval_rewards, epsilon_values, lines_cleared, learning_rates = train_agent(\n",
        "            env, agent,\n",
        "            num_episodes=episodes_per_stage,\n",
        "            eval_freq=episodes_per_stage // 10,\n",
        "            save_freq=episodes_per_stage // 4,\n",
        "            model_dir=model_dir\n",
        "        )\n",
        "\n",
        "        # Store training metrics for this stage\n",
        "        training_metrics[stage] = {\n",
        "            'rewards': all_rewards,\n",
        "            'lengths': all_lengths,\n",
        "            'losses': all_losses,\n",
        "            'eval_rewards': eval_rewards,\n",
        "            'epsilon_values': epsilon_values,\n",
        "            'lines_cleared': lines_cleared,\n",
        "            'learning_rates': learning_rates\n",
        "        }\n",
        "\n",
        "        # Save trained agent\n",
        "        trained_agents[stage] = agent\n",
        "\n",
        "        # Final evaluation + analysis with rendering\n",
        "        print(\"\\nFinal evaluation for this stage:\")\n",
        "        eval_env = TetrisWrapper(render_mode=\"human\", stage=stage)\n",
        "        avg_reward = evaluate_agent(eval_env, agent, num_episodes=3, render=True, verbose=True)\n",
        "\n",
        "        print(f\"\\nAnalyzing agent's decision patterns for stage {stage}...\")\n",
        "        analyze_agent_decisions(agent, eval_env, episodes=3, stage=stage)\n",
        "\n",
        "        # Clean up environments\n",
        "        env.close()\n",
        "        eval_env.close()\n",
        "\n",
        "    return trained_agents, training_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HQYRlTntSwkP"
      },
      "outputs": [],
      "source": [
        "def transfer_weights_between_agents(source_agent, target_agent):\n",
        "    \"\"\"\n",
        "    Transfer matching weights between two agents (same architecture) –\n",
        "    used for curriculum learning stages.\n",
        "\n",
        "    Parameters:\n",
        "    - source_agent: pretrained agent from earlier stage\n",
        "    - target_agent: newly initialized agent\n",
        "    \"\"\"\n",
        "    source_state = source_agent.policy_net.state_dict()\n",
        "    target_state = target_agent.policy_net.state_dict()\n",
        "\n",
        "    # Copy only layers that exist in both and have same shape\n",
        "    for name, param in source_state.items():\n",
        "        if name in target_state and target_state[name].shape == param.shape:\n",
        "            target_state[name] = param.clone()\n",
        "\n",
        "    # Load into target networks\n",
        "    target_agent.policy_net.load_state_dict(target_state)\n",
        "    target_agent.target_net.load_state_dict(target_state)\n",
        "\n",
        "    print(\"✅ Weights transferred successfully between agents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaoIegiNjp88"
      },
      "source": [
        "<div dir = rtl>\n",
        "\n",
        "# (8) ווזואליזציה"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FdDoz38MVTnj"
      },
      "outputs": [],
      "source": [
        "def print_board_state(env, episode):\n",
        "    \"\"\"\n",
        "    Prints a visual representation of the Tetris board at the end of an episode.\n",
        "\n",
        "    Parameters:\n",
        "    - env: the Tetris environment\n",
        "    - episode: the current episode number\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"BOARD STATE AFTER EPISODE {episode}\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    # Get the board state from the environment\n",
        "    field_array = env.tetris.field_array\n",
        "    field_height = env.field_height\n",
        "    field_width = env.field_width\n",
        "\n",
        "    # Create the board representation with colored blocks\n",
        "    # ANSI color codes\n",
        "    colors = {\n",
        "        'T': '\\033[95m',  # Purple\n",
        "        'O': '\\033[93m',  # Yellow\n",
        "        'J': '\\033[94m',  # Blue\n",
        "        'L': '\\033[91m',  # Orange/Red\n",
        "        'I': '\\033[96m',  # Cyan\n",
        "        'S': '\\033[92m',  # Green\n",
        "        'Z': '\\033[91m',  # Red\n",
        "    }\n",
        "    reset = '\\033[0m'  # Reset color\n",
        "\n",
        "    # Print column indices at the top\n",
        "    print(\"  \", end=\"\")\n",
        "    for x in range(field_width):\n",
        "        print(f\"{x}\", end=\" \")\n",
        "    print()\n",
        "\n",
        "    # Print top border\n",
        "    print(\"  \" + \"+\" + \"-\" * (field_width * 2 - 1) + \"+\")\n",
        "\n",
        "    # Print board content\n",
        "    for y in range(field_height):\n",
        "        # Print row index\n",
        "        print(f\"{y:2d}|\", end=\"\")\n",
        "\n",
        "        for x in range(field_width):\n",
        "            cell = field_array[y][x]\n",
        "            if cell:\n",
        "                # Find the Tetromino type for this cell if possible\n",
        "                shape = '█'  # Default filled cell\n",
        "                if hasattr(cell, 'tetromino') and hasattr(cell.tetromino, 'shape'):\n",
        "                    shape = cell.tetromino.shape\n",
        "                    print(f\"{colors.get(shape, '')}{shape}{reset}\", end=\" \")\n",
        "                else:\n",
        "                    print(\"█\", end=\" \")\n",
        "            else:\n",
        "                # Empty cell\n",
        "                print(\"·\", end=\" \")\n",
        "\n",
        "        print(\"|\")\n",
        "\n",
        "    # Print bottom border\n",
        "    print(\"  \" + \"+\" + \"-\" * (field_width * 2 - 1) + \"+\")\n",
        "    print(f\"{'='*40}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "t5WAOSJsSebX"
      },
      "outputs": [],
      "source": [
        "def analyze_agent_decisions(agent, env, episodes=5, stage=None):\n",
        "    \"\"\"\n",
        "    Analyzes and visualizes the agent's decision patterns\n",
        "\n",
        "    Parameters:\n",
        "    - agent: the trained DQN agent\n",
        "    - env: the Tetris environment\n",
        "    - episodes: number of episodes to analyze\n",
        "    - stage: current training stage (for labeling graphs)\n",
        "    \"\"\"\n",
        "    rotation_dist = {0: 0, 1: 0, 2: 0, 3: 0}\n",
        "\n",
        "    # Initialize the x_position_dist based on the actual action space\n",
        "    # Get all possible column values from the action space\n",
        "    all_columns = set(column for column, _ in env.action_space)\n",
        "    x_position_dist = {x: 0 for x in all_columns}\n",
        "\n",
        "    total_actions = 0\n",
        "\n",
        "    stage_text = f\" (Stage {stage})\" if stage is not None else \"\"\n",
        "    print(f\"Collecting data over {episodes} episodes{stage_text}...\")\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            # Select action in evaluation mode (no randomness)\n",
        "            action = agent.select_action(state, eval_mode=True)\n",
        "\n",
        "            # Get the column and rotation from the action\n",
        "            column, rotation = env.action_space[action]\n",
        "\n",
        "            # Update statistics\n",
        "            if rotation in rotation_dist:\n",
        "                rotation_dist[rotation] += 1\n",
        "            else:\n",
        "                rotation_dist[rotation] = 1\n",
        "\n",
        "            if column in x_position_dist:\n",
        "                x_position_dist[column] += 1\n",
        "            else:\n",
        "                x_position_dist[column] = 1\n",
        "\n",
        "            total_actions += 1\n",
        "            steps += 1\n",
        "\n",
        "            # Take action\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "        print(f\"  Episode {episode+1}/{episodes} completed: {steps} steps\")\n",
        "\n",
        "    print(f\"Total actions analyzed: {total_actions}\")\n",
        "\n",
        "    # Convert to percentages\n",
        "    for rot in rotation_dist:\n",
        "        rotation_dist[rot] = (rotation_dist[rot] / total_actions) * 100 if total_actions > 0 else 0\n",
        "\n",
        "    for x in x_position_dist:\n",
        "        x_position_dist[x] = (x_position_dist[x] / total_actions) * 100 if total_actions > 0 else 0\n",
        "\n",
        "    # Create the visualization\n",
        "    print(\"\\nDisplaying agent decision distribution...\")\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Set color theme based on stage\n",
        "    if stage is not None:\n",
        "        # Different color for each stage\n",
        "        colors = ['blue', 'green', 'orange', 'purple', 'red']\n",
        "        color = colors[(stage-1) % len(colors)]\n",
        "    else:\n",
        "        color = 'blue'\n",
        "\n",
        "    # Rotation distribution chart\n",
        "    rotation_labels = [f\"Rotation {i}\" for i in sorted(rotation_dist.keys())]\n",
        "    rotation_values = [rotation_dist[i] for i in sorted(rotation_dist.keys())]\n",
        "    axs[0].bar(rotation_labels, rotation_values, color=color)\n",
        "    axs[0].set_title('Rotation Distribution')\n",
        "    axs[0].set_ylabel('Percentage (%)')\n",
        "    axs[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    for i, v in enumerate(rotation_values):\n",
        "        axs[0].text(i, v + 1, f'{v:.1f}%', ha='center')\n",
        "\n",
        "    # Column distribution chart\n",
        "    x_labels = [f\"Column {i}\" for i in sorted(x_position_dist.keys())]\n",
        "    x_values = [x_position_dist[i] for i in sorted(x_position_dist.keys())]\n",
        "    axs[1].bar(x_labels, x_values, color=color)\n",
        "    axs[1].set_title('Column Distribution')\n",
        "    axs[1].set_ylabel('Percentage (%)')\n",
        "    axs[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    for i, v in enumerate(x_values):\n",
        "        axs[1].text(i, v + 1, f'{v:.1f}%', ha='center')\n",
        "\n",
        "    title = 'Tetris Piece Placement Analysis'\n",
        "    if stage is not None:\n",
        "        title += f' - Stage {stage}'\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'rotation_dist': rotation_dist,\n",
        "        'x_position_dist': x_position_dist\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IzkajFfPj8Sv"
      },
      "outputs": [],
      "source": [
        "def plot_training_progress(rewards, lengths, losses, eval_rewards=None,\n",
        "                          epsilon_values=None, lines_cleared=None, learning_rates=None,\n",
        "                          confidence_values=None, action_counts=None, cell_fill_counts=None,\n",
        "                          smoothing=30, stage=None):\n",
        "    \"\"\"\n",
        "    Plot comprehensive training diagnostics: rewards, lengths, loss, epsilon, lines cleared, learning rate,\n",
        "    action confidence, and heatmaps. Supports per-stage coloring and smoothing.\n",
        "    \"\"\"\n",
        "\n",
        "    def plot_with_confidence(x, y, label='', color='blue', ax=None):\n",
        "        \"\"\"\n",
        "        Helper function: plots a line with a shaded confidence region (std).\n",
        "        \"\"\"\n",
        "        y = np.array(y)\n",
        "        window = smoothing\n",
        "\n",
        "        if len(y) < window:\n",
        "            mean = y\n",
        "            std = np.zeros_like(y)\n",
        "            x = x[:len(y)]\n",
        "        else:\n",
        "            mean = np.convolve(y, np.ones(window)/window, mode='valid')\n",
        "            std = np.array([np.std(y[max(0, i - window):i+1]) for i in range(len(mean))])\n",
        "            x = x[:len(mean)]\n",
        "\n",
        "        if ax is None:\n",
        "            ax = plt.gca()\n",
        "        ax.plot(x, mean, label=label, color=color)\n",
        "        ax.fill_between(x, mean - std, mean + std, alpha=0.3, color=color)\n",
        "        ax.set_title(label)\n",
        "        ax.set_xlabel(\"Episode\")\n",
        "        ax.set_ylabel(\"Value\")\n",
        "        if 'Confidence' in label:\n",
        "            ax.set_ylim(0, 1.05)\n",
        "        ax.grid(True)\n",
        "        ax.legend()\n",
        "\n",
        "    def plot_action_heatmap(ax, cell_fill_counts, stage=5):\n",
        "        \"\"\"\n",
        "        Helper function: draw a heatmap of how often each board cell was filled.\n",
        "        \"\"\"\n",
        "        sns.heatmap(cell_fill_counts, cmap='YlGnBu', annot=False, ax=ax)\n",
        "        ax.set_title(f\"Board Cell Fill Heatmap (Stage {stage})\")\n",
        "        ax.set_xlabel(\"Column\")\n",
        "        ax.set_ylabel(\"Row\")\n",
        "\n",
        "    # Custom colors for each metric\n",
        "    colors = {\n",
        "        'rewards': '#00BFFF', 'rewards_smooth': '#007ACC', 'eval_rewards': '#FF4500',\n",
        "        'lengths': '#32CD32', 'lengths_smooth': '#228B22', 'losses': '#9932CC',\n",
        "        'losses_smooth': '#6A0DAD', 'epsilon': '#FFD700', 'lines': '#FF69B4',\n",
        "        'lines_smooth': '#C71585', 'learning_rate': '#7FFF00'\n",
        "    }\n",
        "\n",
        "    # Optional color adjustment based on stage number\n",
        "    if stage is not None:\n",
        "        stage_factor = 0.6 + (stage * 0.1)\n",
        "        for key in colors:\n",
        "            hex_color = colors[key].lstrip('#')\n",
        "            rgb = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
        "            rgb = tuple(min(255, int(c * stage_factor)) for c in rgb)\n",
        "            colors[key] = '#{:02x}{:02x}{:02x}'.format(*rgb)\n",
        "\n",
        "    stage_text = f\" - Stage {stage}\" if stage is not None else \"\"\n",
        "    plt.figure(figsize=(18, 15))\n",
        "\n",
        "    # 1. Reward plot\n",
        "    plt.subplot(3, 2, 1)\n",
        "    plt.plot(rewards, alpha=0.4, label='Reward', color=colors['rewards'])\n",
        "    if len(rewards) >= smoothing:\n",
        "        smooth_rewards = np.convolve(rewards, np.ones(smoothing)/smoothing, mode='valid')\n",
        "        plt.plot(range(smoothing-1, len(rewards)), smooth_rewards,\n",
        "                 label=f'Smoothed ({smoothing} ep.)',\n",
        "                 color=colors['rewards_smooth'], linewidth=2)\n",
        "    if eval_rewards:\n",
        "        eval_x = np.linspace(0, len(rewards)-1, len(eval_rewards))\n",
        "        plt.plot(eval_x, eval_rewards, color=colors['eval_rewards'],\n",
        "                 label='Evaluation Reward', linewidth=2)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title(f'Reward Progress{stage_text}')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Episode length\n",
        "    plt.subplot(3, 2, 2)\n",
        "    plt.plot(lengths, alpha=0.4, label='Episode Length', color=colors['lengths'])\n",
        "    if len(lengths) >= smoothing:\n",
        "        smooth_lengths = np.convolve(lengths, np.ones(smoothing)/smoothing, mode='valid')\n",
        "        plt.plot(range(smoothing-1, len(lengths)), smooth_lengths,\n",
        "                 label=f'Smoothed ({smoothing} ep.)',\n",
        "                 color=colors['lengths_smooth'], linewidth=2)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Steps')\n",
        "    plt.title(f'Episode Lengths{stage_text}')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Loss\n",
        "    plt.subplot(3, 2, 3)\n",
        "    plt.plot(losses, alpha=0.4, label='Loss', color=colors['losses'])\n",
        "    if len(losses) >= smoothing:\n",
        "        smooth_losses = np.convolve(losses, np.ones(smoothing)/smoothing, mode='valid')\n",
        "        plt.plot(range(smoothing-1, len(losses)), smooth_losses,\n",
        "                 label=f'Smoothed ({smoothing} ep.)',\n",
        "                 color=colors['losses_smooth'], linewidth=2)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'Training Loss{stage_text}')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Epsilon (exploration)\n",
        "    plt.subplot(3, 2, 4)\n",
        "    if epsilon_values:\n",
        "        plt.plot(epsilon_values, label='Epsilon', color=colors['epsilon'], linewidth=2)\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Epsilon Value')\n",
        "        plt.title(f'Exploration Rate (Epsilon){stage_text}')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, \"Epsilon data not available\", ha='center', va='center', transform=plt.gca().transAxes)\n",
        "        plt.title(f'Exploration Rate (Epsilon){stage_text}')\n",
        "\n",
        "    # 5. Lines cleared per episode\n",
        "    plt.subplot(3, 2, 5)\n",
        "    if lines_cleared:\n",
        "        plt.plot(lines_cleared, alpha=0.4, label='Lines per Episode', color=colors['lines'])\n",
        "\n",
        "        if len(lines_cleared) >= smoothing:\n",
        "            smooth_lines = np.convolve(lines_cleared, np.ones(smoothing)/smoothing, mode='valid')\n",
        "            center_shift = smoothing // 2\n",
        "            x_vals = np.arange(center_shift, center_shift + len(smooth_lines))  # Centered x-axis\n",
        "            plt.plot(x_vals, smooth_lines,\n",
        "                    label=f'Smoothed ({smoothing} ep.)',\n",
        "                    color=colors['lines_smooth'], linewidth=2)\n",
        "\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Lines Cleared')\n",
        "        plt.title(f'Lines Cleared per Episode{stage_text}')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, \"Lines cleared data not available\", ha='center', va='center', transform=plt.gca().transAxes)\n",
        "        plt.title(f'Lines Cleared per Episode{stage_text}')\n",
        "\n",
        "    # 6. Learning rate\n",
        "    plt.subplot(3, 2, 6)\n",
        "    if learning_rates:\n",
        "        plt.plot(learning_rates, label='Learning Rate', color=colors['learning_rate'], linewidth=2)\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Learning Rate')\n",
        "        plt.title(f'Learning Rate over Time{stage_text}')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        if max(learning_rates) / min(learning_rates) > 10:\n",
        "            plt.yscale('log')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, \"Learning rate data not available\", ha='center', va='center', transform=plt.gca().transAxes)\n",
        "        plt.title(f'Learning Rate over Time{stage_text}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(f'Tetris RL Training Progress{stage_text}', fontsize=16, y=0.995)\n",
        "    plt.subplots_adjust(top=0.95)\n",
        "    plt.show()\n",
        "\n",
        "    # Optional confidence and heatmap side-by-side\n",
        "    if confidence_values is not None and cell_fill_counts is not None:\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
        "        plot_with_confidence(np.arange(len(confidence_values)), confidence_values, label='Action Confidence', color='orange', ax=axs[0])\n",
        "        plot_action_heatmap(axs[1], cell_fill_counts, stage=stage if stage is not None else 5)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "K_8qPg0GCKLI"
      },
      "outputs": [],
      "source": [
        "def create_action_distribution_plot(agent, env, episodes=100):\n",
        "    \"\"\"\n",
        "    Analyze and visualize the agent's action distribution over multiple episodes.\n",
        "    Produces bar plots for rotation and column position distributions.\n",
        "\n",
        "    Args:\n",
        "        agent: Trained DQN agent.\n",
        "        env: Tetris environment.\n",
        "        episodes: Number of episodes to analyze.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing rotation and position distributions as percentages.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize counters for rotations (0–3) and column positions (0–field_width-1)\n",
        "    rotation_dist = {0: 0, 1: 0, 2: 0, 3: 0}\n",
        "    x_position_dist = {x: 0 for x in range(env.field_width)}\n",
        "    total_actions = 0\n",
        "\n",
        "    print(\"\\nAgent decision analysis – collecting data...\")\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            # Select action using greedy policy (no randomness)\n",
        "            action = agent.select_action(state, eval_mode=True)\n",
        "\n",
        "            # Decode action into (column, rotation)\n",
        "            column, rotation = env.action_space[action]\n",
        "\n",
        "            # Update statistics\n",
        "            rotation_dist[rotation] += 1\n",
        "            x_position_dist[column] += 1\n",
        "            total_actions += 1\n",
        "            steps += 1\n",
        "\n",
        "            # Perform action in environment\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "        print(f\"  Episode {episode + 1}/{episodes} complete: {steps} steps\")\n",
        "\n",
        "    print(f\"Total actions recorded: {total_actions}\")\n",
        "\n",
        "    # Convert raw counts to percentages\n",
        "    for rot in rotation_dist:\n",
        "        rotation_dist[rot] = (rotation_dist[rot] / total_actions) * 100 if total_actions > 0 else 0\n",
        "\n",
        "    for x in x_position_dist:\n",
        "        x_position_dist[x] = (x_position_dist[x] / total_actions) * 100 if total_actions > 0 else 0\n",
        "\n",
        "    # Plotting the distributions\n",
        "    print(\"\\nDisplaying action distribution plots...\")\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Rotation distribution plot\n",
        "    rotation_labels = [f\"Rotation {i}\" for i in sorted(rotation_dist.keys())]\n",
        "    axs[0].bar(rotation_labels, [rotation_dist[i] for i in sorted(rotation_dist.keys())])\n",
        "    axs[0].set_title('Rotation Distribution')\n",
        "    axs[0].set_ylabel('Percentage (%)')\n",
        "    axs[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    for i, v in enumerate([rotation_dist[i] for i in sorted(rotation_dist.keys())]):\n",
        "        axs[0].text(i, v + 1, f'{v:.1f}%', ha='center')\n",
        "\n",
        "    # Column position distribution plot\n",
        "    x_labels = [f\"Col {i}\" for i in sorted(x_position_dist.keys())]\n",
        "    axs[1].bar(x_labels, [x_position_dist[i] for i in sorted(x_position_dist.keys())])\n",
        "    axs[1].set_title('Column Position Distribution')\n",
        "    axs[1].set_ylabel('Percentage (%)')\n",
        "    axs[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    for i, v in enumerate([x_position_dist[i] for i in sorted(x_position_dist.keys())]):\n",
        "        axs[1].text(i, v + 1, f'{v:.1f}%', ha='center')\n",
        "\n",
        "    # Global plot title\n",
        "    plt.suptitle('Agent’s Tetromino Placement Behavior Analysis', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'rotation_dist': rotation_dist,\n",
        "        'x_position_dist': x_position_dist\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ywH9OCKkJ7F"
      },
      "source": [
        "<div dir = rtl>\n",
        "\n",
        "# (9) הליך ראשי"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qrsTGQBMkT_E",
        "outputId": "22d4fac7-8ec8-4c89-e933-1585319c9c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training of reinforcement learning agent for Tetris\n",
            "\n",
            "Initializing stage 5 with board size: 10×20\n",
            "Updated field dimensions from 10×20 to 10×20\n",
            "\n",
            "============================================================\n",
            "STAGE 5 TRAINING\n",
            "============================================================\n",
            "Board size: (10, 20)\n",
            "Allowed shapes: ['T', 'O', 'J', 'L', 'I', 'S', 'Z']\n",
            "Episodes: 20000\n",
            "Transfer learning: True\n",
            "============================================================\n",
            "\n",
            "Initializing stage 5 with board size: 10×20\n",
            "Updated field dimensions from 10×20 to 10×20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-db380940dde0>:38: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=(self.device == 'cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created new agent for stage 5\n",
            "\n",
            "==================================================\n",
            "Starting training for 20000 episodes\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/20000 [00:00<?, ?it/s, Reward=-1186.5, avg100=-1186.5, eps=1.00, lines=0, buffer=0%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating agent... done. Avg reward: 10.25, Avg lines: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 1/20000 [00:00<1:07:05,  4.97it/s, Reward=33.8, avg100=-428.8, eps=1.00, lines=1, buffer=0%]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  New best model saved! Reward: 10.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 5/20000 [00:00<17:55, 18.59it/s, Reward=208.2, avg100=-206.1, eps=1.00, lines=0, buffer=0%] <ipython-input-12-db380940dde0>:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(self.device == 'cuda')):\n",
            "Training:  10%|█         | 2000/20000 [08:39<1:29:41,  3.34it/s, Reward=-163.4, avg100=-25.5, eps=0.91, lines=0, buffer=39%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating agent... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:  10%|█         | 2001/20000 [08:39<1:51:50,  2.68it/s, Reward=-163.4, avg100=-25.5, eps=0.91, lines=0, buffer=39%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done. Avg reward: 13.16, Avg lines: 0.3\n",
            "  New best model saved! Reward: 13.16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|██        | 4000/20000 [19:03<1:52:22,  2.37it/s, Reward=-11.9, avg100=-20.1, eps=0.82, lines=0, buffer=79%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating agent... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:  20%|██        | 4001/20000 [19:04<2:41:12,  1.65it/s, Reward=-11.9, avg100=-20.1, eps=0.82, lines=0, buffer=79%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done. Avg reward: 58.31, Avg lines: 0.7\n",
            "  New best model saved! Reward: 58.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  25%|██▌       | 5000/20000 [24:52<1:11:55,  3.48it/s, Reward=304.1, avg100=-19.8, eps=0.78, lines=0, buffer=100%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "BOARD STATE AFTER EPISODE 5000\n",
            "========================================\n",
            "  0 1 2 3 4 5 6 7 8 9 \n",
            "  +-------------------+\n",
            " 0|· · · · · · · · · · |\n",
            " 1|· · · · · · · · \u001b[93mO\u001b[0m \u001b[93mO\u001b[0m |\n",
            " 2|· · · · · · · · \u001b[93mO\u001b[0m \u001b[93mO\u001b[0m |\n",
            " 3|· · · · · · \u001b[96mI\u001b[0m \u001b[96mI\u001b[0m \u001b[96mI\u001b[0m \u001b[96mI\u001b[0m |\n",
            " 4|· · · · · · \u001b[93mO\u001b[0m \u001b[93mO\u001b[0m · · |\n",
            " 5|· · · · · · \u001b[93mO\u001b[0m \u001b[93mO\u001b[0m · \u001b[91mL\u001b[0m |\n",
            " 6|· · · · · · · \u001b[91mL\u001b[0m \u001b[91mL\u001b[0m \u001b[91mL\u001b[0m |\n",
            " 7|· · · · · · · · \u001b[94mJ\u001b[0m \u001b[94mJ\u001b[0m |\n",
            " 8|· · · · · · · · \u001b[94mJ\u001b[0m · |\n",
            " 9|· · · · · · · · \u001b[94mJ\u001b[0m · |\n",
            "10|· · · · · · · · \u001b[96mI\u001b[0m · |\n",
            "11|· · · · · · · · \u001b[96mI\u001b[0m · |\n",
            "12|· · · · · · · · \u001b[96mI\u001b[0m · |\n",
            "13|· · · · · · · · \u001b[96mI\u001b[0m · |\n",
            "14|· · · · · · · \u001b[93mO\u001b[0m \u001b[93mO\u001b[0m · |\n",
            "15|· · · · · · · \u001b[93mO\u001b[0m \u001b[93mO\u001b[0m \u001b[91mZ\u001b[0m |\n",
            "16|· \u001b[95mT\u001b[0m \u001b[95mT\u001b[0m \u001b[95mT\u001b[0m · · · · \u001b[91mZ\u001b[0m \u001b[91mZ\u001b[0m |\n",
            "17|· · \u001b[95mT\u001b[0m · · · · · \u001b[91mZ\u001b[0m \u001b[94mJ\u001b[0m |\n",
            "18|· \u001b[91mZ\u001b[0m \u001b[91mZ\u001b[0m · · \u001b[94mJ\u001b[0m \u001b[94mJ\u001b[0m \u001b[94mJ\u001b[0m · \u001b[94mJ\u001b[0m |\n",
            "19|· · \u001b[91mZ\u001b[0m \u001b[91mZ\u001b[0m · · · \u001b[94mJ\u001b[0m \u001b[94mJ\u001b[0m \u001b[94mJ\u001b[0m |\n",
            "  +-------------------+\n",
            "========================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  30%|███       | 6000/20000 [31:08<1:35:49,  2.44it/s, Reward=-106.9, avg100=-10.0, eps=0.73, lines=0, buffer=100%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating agent... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:  30%|███       | 6001/20000 [31:08<2:09:37,  1.80it/s, Reward=-106.9, avg100=-10.0, eps=0.73, lines=0, buffer=100%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done. Avg reward: 132.03, Avg lines: 1.7\n",
            "  New best model saved! Reward: 132.03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  40%|████      | 8000/20000 [44:44<1:28:47,  2.25it/s, Reward=38.1, avg100=5.8, eps=0.62, lines=1, buffer=100%] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating agent... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:  40%|████      | 8001/20000 [44:44<2:00:15,  1.66it/s, Reward=38.1, avg100=5.8, eps=0.62, lines=1, buffer=100%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done. Avg reward: 233.15, Avg lines: 2.4\n",
            "  New best model saved! Reward: 233.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  45%|████▍     | 8981/20000 [52:04<1:24:02,  2.19it/s, Reward=45.7, avg100=19.9, eps=0.56, lines=0, buffer=100%]"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function – runs the full training process\n",
        "    \"\"\"\n",
        "    print(\"Starting training of reinforcement learning agent for Tetris\")\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Create environment to determine input size\n",
        "    temp_env = TetrisWrapper(render_mode=None, stage=5)\n",
        "    sample_state = temp_env.reset()[0]\n",
        "    input_size = len(sample_state)\n",
        "    action_size = len(temp_env.action_space)\n",
        "    temp_env.close()\n",
        "\n",
        "    # Staged training\n",
        "    trained_agents, training_metrics = staged_training(\n",
        "        stages=(5,),\n",
        "        episodes_per_stage=20_000,\n",
        "        transfer_learning=True,\n",
        "        input_size=input_size,\n",
        "        action_size=action_size\n",
        "    )\n",
        "\n",
        "    # Demonstrate the trained agent on the full-size board\n",
        "    print(\"\\nDemonstrating the trained agent on the full board:\")\n",
        "    final_env = TetrisWrapper(render_mode=\"human\", stage=5)\n",
        "    final_agent = trained_agents[5]\n",
        "\n",
        "    for i in range(3):\n",
        "        print(f\"\\nDemo game {i+1}:\")\n",
        "        reward = evaluate_agent(final_env, final_agent, num_episodes=1, render=True, verbose=True)\n",
        "        print(f\"Reward: {reward:.2f}\")\n",
        "\n",
        "    print(\"\\nAnalyzing agent's decision patterns...\")\n",
        "    analyze_agent_decisions(final_agent, final_env, episodes=5)\n",
        "\n",
        "    final_env.close()\n",
        "    print(\"\\nTraining process completed successfully!\")\n",
        "    return trained_agents, training_metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trained_agents, training_metrics = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxs6HaSikgJL"
      },
      "source": [
        "<div dir = rtl>\n",
        "\n",
        "\n",
        "# (10) צפייה בסוכן"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def watch_agent_play_gif(model_path, output_name_base=\"tetris_ep\", episodes=5, stage=5, fps=3):\n",
        "    \"\"\"\n",
        "    Runs multiple games of a trained agent on Tetris, saving GIFs only for episodes\n",
        "    where the agent breaks at least 20 lines.\n",
        "    \"\"\"\n",
        "    env = TetrisWrapper(render_mode=None, stage=stage)\n",
        "\n",
        "    # Get input/output dimensions from dummy run\n",
        "    sample_state, _ = env.reset()\n",
        "    temp_agent = DQNAgent(input_size=1, n_actions=1)\n",
        "    input_size = temp_agent.preprocess_state(sample_state).shape[1]\n",
        "    action_size = len(env.action_space)\n",
        "\n",
        "    agent = DQNAgent(input_size=input_size, n_actions=action_size)\n",
        "    agent.load_model(model_path)\n",
        "    agent.policy_net.eval()\n",
        "\n",
        "    shape_colors = {\n",
        "        'I': (0, 255, 255), 'O': (255, 255, 0), 'T': (128, 0, 128),\n",
        "        'S': (0, 255, 0), 'Z': (255, 0, 0), 'J': (0, 0, 255),\n",
        "        'L': (255, 165, 0), 'default': (200, 200, 200)\n",
        "    }\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        frames = []\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        step = 0\n",
        "        lines_in_episode = 0\n",
        "\n",
        "        print(f\"\\n Running Episode {episode + 1}...\")\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state, eval_mode=True)\n",
        "            state, reward, done, _, info = env.step(action)\n",
        "\n",
        "            board_image = env.render_board_as_image()\n",
        "            h, w, _ = board_image.shape\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(7, 8))\n",
        "            ax.imshow(board_image)\n",
        "\n",
        "            lines_this_step = info.get('lines_cleared', 0)\n",
        "            lines_in_episode += lines_this_step\n",
        "            score = info.get('score', 0)\n",
        "\n",
        "            ax.set_title(f\"Step {step + 1} | Score: {score} | Lines: {lines_in_episode}\", fontsize=12)\n",
        "            ax.set_xticks(np.arange(-0.5, w, 1), minor=True)\n",
        "            ax.set_yticks(np.arange(-0.5, h, 1), minor=True)\n",
        "            ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.3)\n",
        "            ax.tick_params(which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
        "\n",
        "            # Preview next piece\n",
        "            next_shape = getattr(env.tetris.next_tetromino, 'shape', None)\n",
        "            if next_shape in TETROMINOES:\n",
        "                shape_matrix = np.atleast_2d(TETROMINOES[next_shape][0])\n",
        "                color = np.array(shape_colors.get(next_shape, shape_colors['default'])) / 255.0\n",
        "                preview_x, preview_y = w + 1, 2\n",
        "\n",
        "                for y, row in enumerate(shape_matrix):\n",
        "                    for x, cell in enumerate(row):\n",
        "                        if cell:\n",
        "                            ax.add_patch(plt.Rectangle((preview_x + x, preview_y + y), 1, 1, color=color, ec='black'))\n",
        "                ax.text(preview_x, preview_y - 1.5, f\"Next: {next_shape}\", fontsize=10)\n",
        "\n",
        "            fig.canvas.draw()\n",
        "            canvas = fig.canvas\n",
        "            image = np.frombuffer(canvas.buffer_rgba(), dtype='uint8').reshape(canvas.get_width_height()[::-1] + (4,))\n",
        "            image = image[..., :3]\n",
        "            frames.append(image)\n",
        "\n",
        "            plt.close(fig)\n",
        "            step += 1\n",
        "\n",
        "        # Save only if lines broken exceed threshold\n",
        "        if lines_in_episode >= 0:\n",
        "            gif_name = f\"{output_name_base}_ep{episode+1}.gif\"\n",
        "            gif_path = f\"/content/drive/My Drive/tetris_game/gif/{gif_name}\"\n",
        "            imageio.mimsave(gif_path, frames, fps=fps)\n",
        "            print(f\"✅ Saved Episode {episode + 1} with {lines_in_episode} lines → {gif_path}\")\n",
        "            display(HTML(f'<img src=\"{gif_path}\" autoplay loop>'))\n",
        "        else:\n",
        "            print(f\"❌ Skipped Episode {episode + 1}: only {lines_in_episode} lines broken.\")"
      ],
      "metadata": {
        "id": "lDC9SGTPfS0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "watch_agent_play_gif(\n",
        "    model_path=\"models/stage_5/best_model.pt\",\n",
        "    episodes=20,\n",
        "    stage=5,\n",
        "    fps=3\n",
        ")"
      ],
      "metadata": {
        "id": "AFQOg542hjBC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "t2dKBgs2aWM7",
        "R-W3aKNUdiHW",
        "vZJblsH3gllY",
        "q-JSpUvpg2-6",
        "i7kP7fwRi_rM",
        "2dpK8WXsjRyP",
        "CaoIegiNjp88",
        "dxs6HaSikgJL"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyOYZV0UAPwbCKTxmOJSx+Kc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}